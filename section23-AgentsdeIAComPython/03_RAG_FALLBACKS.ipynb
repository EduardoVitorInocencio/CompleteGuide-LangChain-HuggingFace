{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG com LCEL\n",
    "\n",
    "Agora vamos para exemplos mais avançados de chains e como podemos reproduzí-las utilizando LCEL.\n",
    "\n",
    "Vamos começar mostrando como fazer o processos de RAG. Para isso, o processo de criação da vector store é exatamente igual ao que vimos no curso de Aplicações com IA com LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRIANDO CHUNKS - Carregando e particionando o documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS = [\n",
    "    \"arquivos\\Explorando o Universo das IAs com Hugging Face.pdf\"\n",
    "]\n",
    "\n",
    "pages = []\n",
    "\n",
    "for path in PATHS:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recur_split = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap = 50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = recur_split.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDINGS - Importando e instanciando o OpenAiEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edinocencio\\AppData\\Local\\Temp\\ipykernel_8548\\22710313.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORAGE - Amrazenando os dados em um VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora na criação da chain é que temos uma mudança. Podemos definir nossa chain manualmente da seguinte forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import  StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from regex import template\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "prompt_template = \"\"\" Reponda as seguintes perguntas ao usuário utilizando apenas o contexto fornecido.\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pergunta: {pergunta}\n",
    "\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# RunnableParallel: Provavelmente é uma classe que permite executar múltiplos componentes em paralelo.\n",
    "# 'pergunta': Um componente que utiliza RunnablePassthrough, que parece ser uma classe que simplesmente passa os dados sem modificações.\n",
    "# 'context': Um componente que utiliza retriever, que deve ser um objeto ou função responsável por recuperar algum tipo de dados ou contexto.\n",
    "\n",
    "setup_and_retrievel = RunnableParallel(\n",
    "    {\n",
    "        'pergunta': RunnablePassthrough(),\n",
    "        'context': retriever\n",
    "     }\n",
    ")\n",
    "\n",
    "chain = setup_and_retrievel | prompt_template | llm | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face é uma empresa que iniciou em 2017 na França, com o desenvolvimento de Chatbots.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"O que é Hugging Face?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entendendo O RunnableParallel e o RunnablePassThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pergunta': 'O que é LangChain?'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'pergunta': RunnablePassthrough().invoke('O que é LangChain?')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pergunta': 'O que é Hugging Face?',\n",
       " 'context': [Document(id='b0af4c33-94b3-41a2-882f-2eb1d53d59c5', metadata={'source': 'arquivos\\\\Explorando o Universo das IAs com Hugging Face.pdf', 'page': 5, 'page_label': '5'}, page_content='Explorando o Universo das IAs com Hugging Face\\n01. O que é Hugging Face?\\nBem-vindos ao curso de Hugging Face da Asimov Academy!\\nNeste curso, vamos explorar as principais utilidades da plataforma de IAHugging Face. Aprenderemos\\ncomo utilizar a plataforma ao máximo, e como incorporar as bibliotecas de Python do Hugging Face\\naos nossos scripts de Python.\\nAfinal, o que é Hugging Face?\\nA Hugging Face é uma empresa que iniciou em 2017 na França, com o desenvolvimento de Chatbots.'),\n",
       "  Document(id='4bdc3bdd-fef9-43c3-a02a-3902b9231c9d', metadata={'source': 'arquivos\\\\Explorando o Universo das IAs com Hugging Face.pdf', 'page': 88, 'page_label': '88'}, page_content='Face.\\nÉ claro que o Hugging Face não acaba aqui. Não tivemos tempo de explorar boa parte das tarefas que\\nexistem no Hugging Face. Dito isso, agora sabemoscomo fazer este processo: conseguimos procurar por\\numa tarefa nova, encontrar o código necessário para baixá-lo (seja pela bibliotecatransformers\\nou alguma outra), avaliar se conseguiremos rodar o modelo localmente, e entender como passar\\nargumentos e obter resultados a partir dele.'),\n",
       "  Document(id='97257724-fdc2-44e8-8acc-958901049366', metadata={'source': 'arquivos\\\\Explorando o Universo das IAs com Hugging Face.pdf', 'page': 6, 'page_label': '6'}, page_content='O Hugging Face cobra apenas se você quiser utilizar a infraestrutura deles para hospedar algum\\nprojeto privado pessoal ou da sua empresa.\\nComo usaremos o Hugging Face?\\nNeste curso, vamos explorar todo o potencial que existe na plataforma do Hugging Face. Isso inclui\\nentender como acessar os modelos, conjuntos de dados, e aplicativos (os chamados “Spaces”) que\\nestão na plataforma. (Note que vamos usar bastante o termo “modelos” , que é um pouco mais técnico\\nque IAs.)'),\n",
       "  Document(id='5085acc8-46bd-4c09-ad9c-1fe0a843b49b', metadata={'source': 'arquivos\\\\Explorando o Universo das IAs com Hugging Face.pdf', 'page': 1, 'page_label': '1'}, page_content='Explorando o Universo das IAs com Hugging Face\\nConteúdo\\n01. O que é Hugging Face? 5\\nAfinal, o que é Hugging Face? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\nÉ tudo aberto mesmo? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\nComo usaremos o Hugging Face? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n02. A plataforma Hugging Face 7')]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel(\n",
    "    {\n",
    "        'pergunta': RunnablePassthrough(),\n",
    "        'context': retriever\n",
    "     }\n",
    ").invoke(\"O que é Hugging Face?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma alternativa não paralelizável\n",
    "\n",
    "Da seguinte forma, també obtemos o mesmo resultado, mas da forma não paralelizável, o que pode gerar um atraso no processamento de nossa chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face é uma empresa que iniciou em 2017 na França, com o desenvolvimento de Chatbots.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_dict = {'pergunta': RunnablePassthrough(), 'context': retriever}\n",
    "chain = setup_dict | prompt_template | llm | output_parser\n",
    "chain.invoke(\"O que é Hugging Face?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FALLBACKS ## ~Caminho alternativo\n",
    "\n",
    "Ao trabalhar com modelos de linguagem, você pode encontrar problemas nas APIs subjacentes, seja por limitação de taxa ou tempo de inatividade. Portante, à medida que você move suas aplicações LLM para produção, torna-se cada vez mais importante proteger-se contra esses problemas. É por isso que introduzimos o conceito de fallbacks, ou alternativas em português.\n",
    "\n",
    "Uma alternativa é um plano substituto que pode ser usado em uma emergência.\n",
    "\n",
    "Criticamente, as alternativas podem ser aplicadas não apenas no nível do LLM, mas em todo o nível executável. Isso é importante porque, muitas vezes, modelos diferentes exigem prompts diferentes. Então, se sua chamada para a OpenAI falhar, você não quer simplesmente enviar o mesmo prompt para Anthropic - você provavelmente vai querer usar um template de prompt diferente e enviar uma versão diferente lá.\n",
    "\n",
    "\n",
    "# Fallbacks para entradas grandes\n",
    "\n",
    "Quando construímos aplicações, precisamos sempre atentar às questões econômicas que envolvem colocar um modelo em produção. Muitas vezes será necessário otimizar custos, evitrando utilizar modelos maiores (mais caros) para problemas simples. Com fallbacks, temos a alternativca de sempre tentar processar e entrada do usuário com um modelo mais complexo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Eu sou o Edward.\" '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "prompt = PromptTemplate.from_template('Resuma o seguinte texto: {texto}')\n",
    "\n",
    "chain_pequena = prompt | llm \n",
    "chain_pequena.invoke({'texto': 'Oi, eu sou o Edward'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_pequena.invoke({'texto': 'Oi, eu sou o Edward' * 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import  ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model = 'gpt-3.5-turbo-0125')\n",
    "prompt = ChatPromptTemplate.from_template('Resuma o seguinte texto: {texto}')\n",
    "\n",
    "chain_grande = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Oi, eu sou o Edward, este é um trecho repetitivo de Edward se apresentando várias vezes.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 6015, 'total_tokens': 6040, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7724a63f-3674-4942-88a5-c876ffcb7b45-0', usage_metadata={'input_tokens': 6015, 'output_tokens': 25, 'total_tokens': 6040, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_grande.invoke({'texto': 'Oi, eu sou o Edward' * 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_fallback = chain_pequena.with_fallbacks(chain_grande)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
